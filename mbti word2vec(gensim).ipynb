{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"C:/Users/82104/Desktop/객체 팀플/dataset/mbti_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks|||What has been the most life-changing experience in your life?|||http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.|||May the PerC Experience immerse you.|||The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~   http://vimeo.com/22842206|||Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as...|||84389  84390  http://wallpaperpassion.com/upload/23700/friendship-boy-and-girl-wallpaper.jpg  http://assets.dornob.com/wp-content/uploads/2010/04/round-home-design.jpg ...|||Welcome and stuff.|||http://playeressence.com/wp-content/uploads/2013/08/RED-red-the-pokemon-master-32560474-450-338.jpg  Game. Set. Match.|||Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...|||Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...|||All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...|||Dear ENFP:  What were your favorite video games growing up and what are your now, current favorite video games? :cool:|||https://www.youtube.com/watch?v=QyPqT8umzmY|||It appears to be too late. :sad:|||There's someone out there for everyone.|||Wait... I thought confidence was a good thing.|||I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to...|||Yo entp ladies... if you're into a complimentary personality,well, hey.|||... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.|||http://www.youtube.com/watch?v=gDhy7rdfm14  I really dig the part from 1:46 to 2:50|||http://www.youtube.com/watch?v=msqXffgh7b8|||Banned because this thread requires it of me.|||Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.|||http://www.youtube.com/watch?v=Mw7eoU3BMbE|||http://www.youtube.com/watch?v=4V2uYORhQOk|||http://www.youtube.com/watch?v=SlVmgFQQ0TI|||Banned for too many b's in that sentence. How could you! Think of the B!|||Banned for watching movies in the corner with the dunces.|||Banned because Health class clearly taught you nothing about peer pressure.|||Banned for a whole host of reasons!|||http://www.youtube.com/watch?v=IRcrv41hgz4|||1) Two baby deer on left and right munching on a beetle in the middle.  2) Using their own blood, two cavemen diary today's latest happenings on their designated cave diary wall.  3) I see it as...|||a pokemon world  an infj society  everyone becomes an optimist|||49142|||http://www.youtube.com/watch?v=ZRCEq_JFeFM|||http://discovermagazine.com/2012/jul-aug/20-things-you-didnt-know-about-deserts/desert.jpg|||http://oyster.ignimgs.com/mediawiki/apis.ign.com/pokemon-silver-version/d/dd/Ditto.gif|||http://www.serebii.net/potw-dp/Scizor.jpg|||Not all artists are artists because they draw. It's the idea that counts in forming something of your own... like a signature.|||Welcome to the robot ranks, person who downed my self-esteem cuz I'm not an avid signature artist like herself. :proud:|||Banned for taking all the room under my bed. Ya gotta learn to share with the roaches.|||http://www.youtube.com/watch?v=w8IgImn57aQ|||Banned for being too much of a thundering, grumbling kind of storm... yep.|||Ahh... old high school music I haven't heard in ages.   http://www.youtube.com/watch?v=dcCRUPCdB1w|||I failed a public speaking class a few years ago and I've sort of learned what I could do better were I to be in that position again. A big part of my failure was just overloading myself with too...|||I like this person's mentality. He's a confirmed INTJ by the way. http://www.youtube.com/watch?v=hGKLI-GEc6M|||Move to the Denver area and start a new life for myself.'\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.posts.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    regex = re.compile('[%s]' % re.escape('|'))\n",
    "    text = regex.sub(\" \", text)\n",
    "    words = str(text).split()\n",
    "    words = [i.lower() + \" \" for i in words]\n",
    "    words = [i for i in words if not \"http\" in i]\n",
    "    words = \" \".join(words)\n",
    "    words = words.translate(words.maketrans('', '', string.punctuation))\n",
    "    return words\n",
    "\n",
    "def word_text(text):\n",
    "    words = str(text).split()        # 전처리한 텍스트를 단어로 쪼갬 ,word2vec 함수에 넣기 위해서\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = np.unique(data.type.values)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_index(string):\n",
    "    return list(types).index(string)                          #위의 16개의 mbti 유형 값들을 리스트화해서 인덱스를 반납\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['type_index'] = data['type'].apply(get_type_index)         #데이터 type을 get type index 함수를 이용해 type index 값을 생성\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text']=data['posts'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enfp  and  intj  moments  sportscenter  not  top  ten  plays  pranks  what  has  been  the  most  lifechanging  experience  in  your  life  on  repeat  for  most  of  today  may  the  perc  experience  immerse  you  the  last  thing  my  infj  friend  posted  on  his  facebook  before  committing  suicide  the  next  day  rest  in  peace  hello  enfj7  sorry  to  hear  of  your  distress  its  only  natural  for  a  relationship  to  not  be  perfection  all  the  time  in  every  moment  of  existence  try  to  figure  the  hard  times  as  times  of  growth  as  84389  84390    welcome  and  stuff  game  set  match  prozac  wellbrutin  at  least  thirty  minutes  of  moving  your  legs  and  i  dont  mean  moving  them  while  sitting  in  your  same  desk  chair  weed  in  moderation  maybe  try  edibles  as  a  healthier  alternative  basically  come  up  with  three  items  youve  determined  that  each  type  or  whichever  types  you  want  to  do  would  more  than  likely  use  given  each  types  cognitive  functions  and  whatnot  when  left  by  all  things  in  moderation  sims  is  indeed  a  video  game  and  a  good  one  at  that  note  a  good  one  at  that  is  somewhat  subjective  in  that  i  am  not  completely  promoting  the  death  of  any  given  sim  dear  enfp  what  were  your  favorite  video  games  growing  up  and  what  are  your  now  current  favorite  video  games  cool  it  appears  to  be  too  late  sad  theres  someone  out  there  for  everyone  wait  i  thought  confidence  was  a  good  thing  i  just  cherish  the  time  of  solitude  bc  i  revel  within  my  inner  world  more  whereas  most  other  time  id  be  workin  just  enjoy  the  me  time  while  you  can  dont  worry  people  will  always  be  around  to  yo  entp  ladies  if  youre  into  a  complimentary  personalitywell  hey    when  your  main  social  outlet  is  xbox  live  conversations  and  even  then  you  verbally  fatigue  quickly  i  really  dig  the  part  from  146  to  250  banned  because  this  thread  requires  it  of  me  get  high  in  backyard  roast  and  eat  marshmellows  in  backyard  while  conversing  over  something  intellectual  followed  by  massages  and  kisses  banned  for  too  many  bs  in  that  sentence  how  could  you  think  of  the  b  banned  for  watching  movies  in  the  corner  with  the  dunces  banned  because  health  class  clearly  taught  you  nothing  about  peer  pressure  banned  for  a  whole  host  of  reasons  1  two  baby  deer  on  left  and  right  munching  on  a  beetle  in  the  middle  2  using  their  own  blood  two  cavemen  diary  todays  latest  happenings  on  their  designated  cave  diary  wall  3  i  see  it  as  a  pokemon  world  an  infj  society  everyone  becomes  an  optimist  49142  not  all  artists  are  artists  because  they  draw  its  the  idea  that  counts  in  forming  something  of  your  own  like  a  signature  welcome  to  the  robot  ranks  person  who  downed  my  selfesteem  cuz  im  not  an  avid  signature  artist  like  herself  proud  banned  for  taking  all  the  room  under  my  bed  ya  gotta  learn  to  share  with  the  roaches  banned  for  being  too  much  of  a  thundering  grumbling  kind  of  storm  yep  ahh  old  high  school  music  i  havent  heard  in  ages  i  failed  a  public  speaking  class  a  few  years  ago  and  ive  sort  of  learned  what  i  could  do  better  were  i  to  be  in  that  position  again  a  big  part  of  my  failure  was  just  overloading  myself  with  too  i  like  this  persons  mentality  hes  a  confirmed  intj  by  the  way  move  to  the  denver  area  and  start  a  new  life  for  myself '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cleaned_text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>type_index</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>8</td>\n",
       "      <td>enfp  and  intj  moments  sportscenter  not  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>3</td>\n",
       "      <td>im  finding  the  lack  of  me  in  these  pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>11</td>\n",
       "      <td>good  one    of  course  to  which  i  say  i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>10</td>\n",
       "      <td>dear  intp  i  enjoyed  our  conversation  the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>2</td>\n",
       "      <td>youre  fired  thats  another  silly  misconcep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  type_index  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...           8   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...           3   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...          11   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...          10   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...           2   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  enfp  and  intj  moments  sportscenter  not  t...  \n",
       "1  im  finding  the  lack  of  me  in  these  pos...  \n",
       "2  good  one    of  course  to  which  i  say  i ...  \n",
       "3  dear  intp  i  enjoyed  our  conversation  the...  \n",
       "4  youre  fired  thats  another  silly  misconcep...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['wording_text']=data['cleaned_text'].apply(word_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enfp',\n",
       " 'and',\n",
       " 'intj',\n",
       " 'moments',\n",
       " 'sportscenter',\n",
       " 'not',\n",
       " 'top',\n",
       " 'ten',\n",
       " 'plays',\n",
       " 'pranks',\n",
       " 'what',\n",
       " 'has',\n",
       " 'been',\n",
       " 'the',\n",
       " 'most',\n",
       " 'lifechanging',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'your',\n",
       " 'life',\n",
       " 'on',\n",
       " 'repeat',\n",
       " 'for',\n",
       " 'most',\n",
       " 'of',\n",
       " 'today',\n",
       " 'may',\n",
       " 'the',\n",
       " 'perc',\n",
       " 'experience',\n",
       " 'immerse',\n",
       " 'you',\n",
       " 'the',\n",
       " 'last',\n",
       " 'thing',\n",
       " 'my',\n",
       " 'infj',\n",
       " 'friend',\n",
       " 'posted',\n",
       " 'on',\n",
       " 'his',\n",
       " 'facebook',\n",
       " 'before',\n",
       " 'committing',\n",
       " 'suicide',\n",
       " 'the',\n",
       " 'next',\n",
       " 'day',\n",
       " 'rest',\n",
       " 'in',\n",
       " 'peace',\n",
       " 'hello',\n",
       " 'enfj7',\n",
       " 'sorry',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'of',\n",
       " 'your',\n",
       " 'distress',\n",
       " 'its',\n",
       " 'only',\n",
       " 'natural',\n",
       " 'for',\n",
       " 'a',\n",
       " 'relationship',\n",
       " 'to',\n",
       " 'not',\n",
       " 'be',\n",
       " 'perfection',\n",
       " 'all',\n",
       " 'the',\n",
       " 'time',\n",
       " 'in',\n",
       " 'every',\n",
       " 'moment',\n",
       " 'of',\n",
       " 'existence',\n",
       " 'try',\n",
       " 'to',\n",
       " 'figure',\n",
       " 'the',\n",
       " 'hard',\n",
       " 'times',\n",
       " 'as',\n",
       " 'times',\n",
       " 'of',\n",
       " 'growth',\n",
       " 'as',\n",
       " '84389',\n",
       " '84390',\n",
       " 'welcome',\n",
       " 'and',\n",
       " 'stuff',\n",
       " 'game',\n",
       " 'set',\n",
       " 'match',\n",
       " 'prozac',\n",
       " 'wellbrutin',\n",
       " 'at',\n",
       " 'least',\n",
       " 'thirty',\n",
       " 'minutes',\n",
       " 'of',\n",
       " 'moving',\n",
       " 'your',\n",
       " 'legs',\n",
       " 'and',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'mean',\n",
       " 'moving',\n",
       " 'them',\n",
       " 'while',\n",
       " 'sitting',\n",
       " 'in',\n",
       " 'your',\n",
       " 'same',\n",
       " 'desk',\n",
       " 'chair',\n",
       " 'weed',\n",
       " 'in',\n",
       " 'moderation',\n",
       " 'maybe',\n",
       " 'try',\n",
       " 'edibles',\n",
       " 'as',\n",
       " 'a',\n",
       " 'healthier',\n",
       " 'alternative',\n",
       " 'basically',\n",
       " 'come',\n",
       " 'up',\n",
       " 'with',\n",
       " 'three',\n",
       " 'items',\n",
       " 'youve',\n",
       " 'determined',\n",
       " 'that',\n",
       " 'each',\n",
       " 'type',\n",
       " 'or',\n",
       " 'whichever',\n",
       " 'types',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'do',\n",
       " 'would',\n",
       " 'more',\n",
       " 'than',\n",
       " 'likely',\n",
       " 'use',\n",
       " 'given',\n",
       " 'each',\n",
       " 'types',\n",
       " 'cognitive',\n",
       " 'functions',\n",
       " 'and',\n",
       " 'whatnot',\n",
       " 'when',\n",
       " 'left',\n",
       " 'by',\n",
       " 'all',\n",
       " 'things',\n",
       " 'in',\n",
       " 'moderation',\n",
       " 'sims',\n",
       " 'is',\n",
       " 'indeed',\n",
       " 'a',\n",
       " 'video',\n",
       " 'game',\n",
       " 'and',\n",
       " 'a',\n",
       " 'good',\n",
       " 'one',\n",
       " 'at',\n",
       " 'that',\n",
       " 'note',\n",
       " 'a',\n",
       " 'good',\n",
       " 'one',\n",
       " 'at',\n",
       " 'that',\n",
       " 'is',\n",
       " 'somewhat',\n",
       " 'subjective',\n",
       " 'in',\n",
       " 'that',\n",
       " 'i',\n",
       " 'am',\n",
       " 'not',\n",
       " 'completely',\n",
       " 'promoting',\n",
       " 'the',\n",
       " 'death',\n",
       " 'of',\n",
       " 'any',\n",
       " 'given',\n",
       " 'sim',\n",
       " 'dear',\n",
       " 'enfp',\n",
       " 'what',\n",
       " 'were',\n",
       " 'your',\n",
       " 'favorite',\n",
       " 'video',\n",
       " 'games',\n",
       " 'growing',\n",
       " 'up',\n",
       " 'and',\n",
       " 'what',\n",
       " 'are',\n",
       " 'your',\n",
       " 'now',\n",
       " 'current',\n",
       " 'favorite',\n",
       " 'video',\n",
       " 'games',\n",
       " 'cool',\n",
       " 'it',\n",
       " 'appears',\n",
       " 'to',\n",
       " 'be',\n",
       " 'too',\n",
       " 'late',\n",
       " 'sad',\n",
       " 'theres',\n",
       " 'someone',\n",
       " 'out',\n",
       " 'there',\n",
       " 'for',\n",
       " 'everyone',\n",
       " 'wait',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'confidence',\n",
       " 'was',\n",
       " 'a',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'i',\n",
       " 'just',\n",
       " 'cherish',\n",
       " 'the',\n",
       " 'time',\n",
       " 'of',\n",
       " 'solitude',\n",
       " 'bc',\n",
       " 'i',\n",
       " 'revel',\n",
       " 'within',\n",
       " 'my',\n",
       " 'inner',\n",
       " 'world',\n",
       " 'more',\n",
       " 'whereas',\n",
       " 'most',\n",
       " 'other',\n",
       " 'time',\n",
       " 'id',\n",
       " 'be',\n",
       " 'workin',\n",
       " 'just',\n",
       " 'enjoy',\n",
       " 'the',\n",
       " 'me',\n",
       " 'time',\n",
       " 'while',\n",
       " 'you',\n",
       " 'can',\n",
       " 'dont',\n",
       " 'worry',\n",
       " 'people',\n",
       " 'will',\n",
       " 'always',\n",
       " 'be',\n",
       " 'around',\n",
       " 'to',\n",
       " 'yo',\n",
       " 'entp',\n",
       " 'ladies',\n",
       " 'if',\n",
       " 'youre',\n",
       " 'into',\n",
       " 'a',\n",
       " 'complimentary',\n",
       " 'personalitywell',\n",
       " 'hey',\n",
       " 'when',\n",
       " 'your',\n",
       " 'main',\n",
       " 'social',\n",
       " 'outlet',\n",
       " 'is',\n",
       " 'xbox',\n",
       " 'live',\n",
       " 'conversations',\n",
       " 'and',\n",
       " 'even',\n",
       " 'then',\n",
       " 'you',\n",
       " 'verbally',\n",
       " 'fatigue',\n",
       " 'quickly',\n",
       " 'i',\n",
       " 'really',\n",
       " 'dig',\n",
       " 'the',\n",
       " 'part',\n",
       " 'from',\n",
       " '146',\n",
       " 'to',\n",
       " '250',\n",
       " 'banned',\n",
       " 'because',\n",
       " 'this',\n",
       " 'thread',\n",
       " 'requires',\n",
       " 'it',\n",
       " 'of',\n",
       " 'me',\n",
       " 'get',\n",
       " 'high',\n",
       " 'in',\n",
       " 'backyard',\n",
       " 'roast',\n",
       " 'and',\n",
       " 'eat',\n",
       " 'marshmellows',\n",
       " 'in',\n",
       " 'backyard',\n",
       " 'while',\n",
       " 'conversing',\n",
       " 'over',\n",
       " 'something',\n",
       " 'intellectual',\n",
       " 'followed',\n",
       " 'by',\n",
       " 'massages',\n",
       " 'and',\n",
       " 'kisses',\n",
       " 'banned',\n",
       " 'for',\n",
       " 'too',\n",
       " 'many',\n",
       " 'bs',\n",
       " 'in',\n",
       " 'that',\n",
       " 'sentence',\n",
       " 'how',\n",
       " 'could',\n",
       " 'you',\n",
       " 'think',\n",
       " 'of',\n",
       " 'the',\n",
       " 'b',\n",
       " 'banned',\n",
       " 'for',\n",
       " 'watching',\n",
       " 'movies',\n",
       " 'in',\n",
       " 'the',\n",
       " 'corner',\n",
       " 'with',\n",
       " 'the',\n",
       " 'dunces',\n",
       " 'banned',\n",
       " 'because',\n",
       " 'health',\n",
       " 'class',\n",
       " 'clearly',\n",
       " 'taught',\n",
       " 'you',\n",
       " 'nothing',\n",
       " 'about',\n",
       " 'peer',\n",
       " 'pressure',\n",
       " 'banned',\n",
       " 'for',\n",
       " 'a',\n",
       " 'whole',\n",
       " 'host',\n",
       " 'of',\n",
       " 'reasons',\n",
       " '1',\n",
       " 'two',\n",
       " 'baby',\n",
       " 'deer',\n",
       " 'on',\n",
       " 'left',\n",
       " 'and',\n",
       " 'right',\n",
       " 'munching',\n",
       " 'on',\n",
       " 'a',\n",
       " 'beetle',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " '2',\n",
       " 'using',\n",
       " 'their',\n",
       " 'own',\n",
       " 'blood',\n",
       " 'two',\n",
       " 'cavemen',\n",
       " 'diary',\n",
       " 'todays',\n",
       " 'latest',\n",
       " 'happenings',\n",
       " 'on',\n",
       " 'their',\n",
       " 'designated',\n",
       " 'cave',\n",
       " 'diary',\n",
       " 'wall',\n",
       " '3',\n",
       " 'i',\n",
       " 'see',\n",
       " 'it',\n",
       " 'as',\n",
       " 'a',\n",
       " 'pokemon',\n",
       " 'world',\n",
       " 'an',\n",
       " 'infj',\n",
       " 'society',\n",
       " 'everyone',\n",
       " 'becomes',\n",
       " 'an',\n",
       " 'optimist',\n",
       " '49142',\n",
       " 'not',\n",
       " 'all',\n",
       " 'artists',\n",
       " 'are',\n",
       " 'artists',\n",
       " 'because',\n",
       " 'they',\n",
       " 'draw',\n",
       " 'its',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'that',\n",
       " 'counts',\n",
       " 'in',\n",
       " 'forming',\n",
       " 'something',\n",
       " 'of',\n",
       " 'your',\n",
       " 'own',\n",
       " 'like',\n",
       " 'a',\n",
       " 'signature',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'robot',\n",
       " 'ranks',\n",
       " 'person',\n",
       " 'who',\n",
       " 'downed',\n",
       " 'my',\n",
       " 'selfesteem',\n",
       " 'cuz',\n",
       " 'im',\n",
       " 'not',\n",
       " 'an',\n",
       " 'avid',\n",
       " 'signature',\n",
       " 'artist',\n",
       " 'like',\n",
       " 'herself',\n",
       " 'proud',\n",
       " 'banned',\n",
       " 'for',\n",
       " 'taking',\n",
       " 'all',\n",
       " 'the',\n",
       " 'room',\n",
       " 'under',\n",
       " 'my',\n",
       " 'bed',\n",
       " 'ya',\n",
       " 'gotta',\n",
       " 'learn',\n",
       " 'to',\n",
       " 'share',\n",
       " 'with',\n",
       " 'the',\n",
       " 'roaches',\n",
       " 'banned',\n",
       " 'for',\n",
       " 'being',\n",
       " 'too',\n",
       " 'much',\n",
       " 'of',\n",
       " 'a',\n",
       " 'thundering',\n",
       " 'grumbling',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'storm',\n",
       " 'yep',\n",
       " 'ahh',\n",
       " 'old',\n",
       " 'high',\n",
       " 'school',\n",
       " 'music',\n",
       " 'i',\n",
       " 'havent',\n",
       " 'heard',\n",
       " 'in',\n",
       " 'ages',\n",
       " 'i',\n",
       " 'failed',\n",
       " 'a',\n",
       " 'public',\n",
       " 'speaking',\n",
       " 'class',\n",
       " 'a',\n",
       " 'few',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'ive',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'learned',\n",
       " 'what',\n",
       " 'i',\n",
       " 'could',\n",
       " 'do',\n",
       " 'better',\n",
       " 'were',\n",
       " 'i',\n",
       " 'to',\n",
       " 'be',\n",
       " 'in',\n",
       " 'that',\n",
       " 'position',\n",
       " 'again',\n",
       " 'a',\n",
       " 'big',\n",
       " 'part',\n",
       " 'of',\n",
       " 'my',\n",
       " 'failure',\n",
       " 'was',\n",
       " 'just',\n",
       " 'overloading',\n",
       " 'myself',\n",
       " 'with',\n",
       " 'too',\n",
       " 'i',\n",
       " 'like',\n",
       " 'this',\n",
       " 'persons',\n",
       " 'mentality',\n",
       " 'hes',\n",
       " 'a',\n",
       " 'confirmed',\n",
       " 'intj',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " 'move',\n",
       " 'to',\n",
       " 'the',\n",
       " 'denver',\n",
       " 'area',\n",
       " 'and',\n",
       " 'start',\n",
       " 'a',\n",
       " 'new',\n",
       " 'life',\n",
       " 'for',\n",
       " 'myself']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.wording_text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>type_index</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>wording_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>8</td>\n",
       "      <td>enfp  and  intj  moments  sportscenter  not  t...</td>\n",
       "      <td>[enfp, and, intj, moments, sportscenter, not, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>3</td>\n",
       "      <td>im  finding  the  lack  of  me  in  these  pos...</td>\n",
       "      <td>[im, finding, the, lack, of, me, in, these, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>11</td>\n",
       "      <td>good  one    of  course  to  which  i  say  i ...</td>\n",
       "      <td>[good, one, of, course, to, which, i, say, i, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>10</td>\n",
       "      <td>dear  intp  i  enjoyed  our  conversation  the...</td>\n",
       "      <td>[dear, intp, i, enjoyed, our, conversation, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>2</td>\n",
       "      <td>youre  fired  thats  another  silly  misconcep...</td>\n",
       "      <td>[youre, fired, thats, another, silly, misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  type_index  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...           8   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...           3   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...          11   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...          10   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...           2   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  enfp  and  intj  moments  sportscenter  not  t...   \n",
       "1  im  finding  the  lack  of  me  in  these  pos...   \n",
       "2  good  one    of  course  to  which  i  say  i ...   \n",
       "3  dear  intp  i  enjoyed  our  conversation  the...   \n",
       "4  youre  fired  thats  another  silly  misconcep...   \n",
       "\n",
       "                                        wording_text  \n",
       "0  [enfp, and, intj, moments, sportscenter, not, ...  \n",
       "1  [im, finding, the, lack, of, me, in, these, po...  \n",
       "2  [good, one, of, course, to, which, i, say, i, ...  \n",
       "3  [dear, intp, i, enjoyed, our, conversation, th...  \n",
       "4  [youre, fired, thats, another, silly, misconce...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test=train_test_split(data)                               #데이터를 train, test 로 나눔\n",
    "train, val=train_test_split(train)                               # train 데이터를 train, valid로 나눔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size=10000                                                       #사용할 단어 집합의 크기\n",
    "trunc_type=\"post\"\n",
    "pad_type=\"post\"\n",
    "#oov_tok=\"<OOV>\"  # oov_token: 값을 주게 되면, 해당 값은 단어집합에 없는 단어를 대체할 word_index가 된다\n",
    " # 함수 보면 단어 집합 크기 만개인데 만개의 원소 안에 없는 단어가 나오면 인덱스 값을 그냥 1로 할당\n",
    "                                                   \n",
    "tokenizer=Tokenizer(num_words=vocab_size)          #가장 빈도수가 높은 10000개 단어만 선택/oov_token\n",
    "tokenizer.fit_on_texts(data.cleaned_text.values)                      #단어 인덱스 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2_model = Word2Vec(sentences=data.wording_text.values, size=300, window=2, min_count=1)  \n",
    "#임베딩되는 벡터 차원 300/앞 단어 2개, 뒷단어 2개로 타깃 예측/ /최소 빈도수 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lonely', 0.6987595558166504),\n",
       " ('happy', 0.6598274111747742),\n",
       " ('scary', 0.6567836999893188),\n",
       " ('depressed', 0.6402886509895325),\n",
       " ('frustrating', 0.6291552782058716),\n",
       " ('angry', 0.6285240650177002),\n",
       " ('upset', 0.6079093217849731),\n",
       " ('creepy', 0.6057907938957214),\n",
       " ('depressing', 0.6049894094467163),\n",
       " ('hungry', 0.6014245748519897)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2_model.wv.most_similar('sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_tok = tokenizer.texts_to_sequences(train.cleaned_text.values)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_tok_pad = pad_sequences(text_train_tok, maxlen=1500,\n",
    "                                   truncating=trunc_type,padding=pad_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82104\\anaconda3\\envs\\tf2.3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def w2v_to_keras_weights(model, vocab):\n",
    "    vocab_size = len(vocab) + 1\n",
    "    weight_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = model[word]\n",
    "    return weight_matrix\n",
    "\n",
    "embedding_vectors = w2v_to_keras_weights(w2_model, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,Dropout,Conv1D, GlobalMaxPooling1D, Flatten,Bidirectional,TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_vectors):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_vectors.shape[0], \n",
    "                        output_dim=embedding_vectors.shape[1],\n",
    "                        weights=[embedding_vectors], \n",
    "                        input_length=1500, \n",
    "                        trainable=False))\n",
    "    model.add(LSTM(units=32,return_sequences=True))\n",
    "    model.add (TimeDistributed(Dense(32)))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(16, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1500, 300)         47760000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1500, 32)          42624     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 1500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 47,812,528\n",
      "Trainable params: 52,528\n",
      "Non-trainable params: 47,760,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(embedding_vectors = embedding_vectors)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = tf.keras.utils.to_categorical(train.type_index.values, num_classes=16)\n",
    "val_labels= tf.keras.utils.to_categorical(val.type_index.values, num_classes=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "122/122 [==============================] - 215s 2s/step - loss: 2.3781 - acc: 0.2088 - val_loss: 2.2809 - val_acc: 0.1660\n",
      "Epoch 2/30\n",
      "122/122 [==============================] - 199s 2s/step - loss: 2.2770 - acc: 0.2160 - val_loss: 2.2723 - val_acc: 0.2213\n",
      "Epoch 3/30\n",
      "122/122 [==============================] - 199s 2s/step - loss: 2.2718 - acc: 0.2111 - val_loss: 2.2727 - val_acc: 0.2234\n",
      "Epoch 4/30\n",
      "122/122 [==============================] - 197s 2s/step - loss: 2.2684 - acc: 0.2226 - val_loss: 2.2704 - val_acc: 0.2111\n",
      "Epoch 5/30\n",
      "122/122 [==============================] - 200s 2s/step - loss: 2.2638 - acc: 0.2237 - val_loss: 2.2695 - val_acc: 0.2141\n",
      "Epoch 6/30\n",
      "122/122 [==============================] - 198s 2s/step - loss: 2.2502 - acc: 0.2265 - val_loss: 2.2824 - val_acc: 0.2121\n",
      "Epoch 7/30\n",
      "122/122 [==============================] - 197s 2s/step - loss: 2.2378 - acc: 0.2375 - val_loss: 2.2846 - val_acc: 0.2121\n",
      "Epoch 8/30\n",
      "122/122 [==============================] - 196s 2s/step - loss: 2.2245 - acc: 0.2370 - val_loss: 2.2913 - val_acc: 0.2111\n",
      "Epoch 9/30\n",
      "122/122 [==============================] - 226s 2s/step - loss: 2.1969 - acc: 0.2470 - val_loss: 2.3007 - val_acc: 0.2182\n",
      "Epoch 10/30\n",
      "122/122 [==============================] - 197s 2s/step - loss: 2.1686 - acc: 0.2572 - val_loss: 2.3191 - val_acc: 0.2059\n",
      "Epoch 11/30\n",
      "122/122 [==============================] - 197s 2s/step - loss: 2.1330 - acc: 0.2698 - val_loss: 2.3394 - val_acc: 0.2008\n",
      "Epoch 12/30\n",
      "122/122 [==============================] - 197s 2s/step - loss: 2.1052 - acc: 0.2788 - val_loss: 2.3439 - val_acc: 0.2172\n",
      "Epoch 13/30\n",
      "122/122 [==============================] - 200s 2s/step - loss: 2.0723 - acc: 0.2916 - val_loss: 2.3682 - val_acc: 0.2100\n",
      "Epoch 14/30\n",
      "122/122 [==============================] - 196s 2s/step - loss: 2.0423 - acc: 0.3021 - val_loss: 2.3882 - val_acc: 0.2080\n",
      "Epoch 15/30\n",
      "122/122 [==============================] - 196s 2s/step - loss: 2.0163 - acc: 0.3108 - val_loss: 2.4111 - val_acc: 0.2152\n",
      "Epoch 16/30\n",
      "122/122 [==============================] - 195s 2s/step - loss: 1.9980 - acc: 0.3200 - val_loss: 2.4446 - val_acc: 0.2162\n",
      "Epoch 17/30\n",
      "122/122 [==============================] - 197s 2s/step - loss: 1.9704 - acc: 0.3269 - val_loss: 2.4684 - val_acc: 0.2090\n",
      "Epoch 18/30\n",
      "122/122 [==============================] - 207s 2s/step - loss: 1.9592 - acc: 0.3356 - val_loss: 2.4728 - val_acc: 0.2090\n",
      "Epoch 19/30\n",
      "122/122 [==============================] - 198s 2s/step - loss: 1.9448 - acc: 0.3400 - val_loss: 2.4932 - val_acc: 0.2018\n",
      "Epoch 20/30\n",
      "122/122 [==============================] - 201s 2s/step - loss: 1.9260 - acc: 0.3454 - val_loss: 2.5267 - val_acc: 0.2080\n",
      "Epoch 21/30\n",
      "119/122 [============================>.] - ETA: 4s - loss: 1.9145 - acc: 0.3527"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-0fcbccfc66fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model.fit(text_train_tok_pad,one_hot_labels, validation_split=0.2, \n\u001b[1;32m----> 2\u001b[1;33m                     epochs=30,batch_size=32, verbose = 1)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(text_train_tok_pad,one_hot_labels, validation_split=0.2, \n",
    "                    epochs=30,batch_size=32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2.3] *",
   "language": "python",
   "name": "conda-env-tf2.3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
